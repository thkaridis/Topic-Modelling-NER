{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling & NER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mscuser/anaconda2/lib/python2.7/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import codecs\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pprint\n",
    "from gensim.models import Phrases\n",
    "from gensim import models, corpora\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above all the NIPS papers are going to be inserted into a list, ignoring all the characters that provoke encoding errors. The number of docs should be 1740."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = 'nipstxt/'\n",
    "years = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "subDirs = ['nips' + yr for yr in years]\n",
    "docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n"
     ]
    }
   ],
   "source": [
    "for element in subDirs:\n",
    "    doc_files = os.listdir(dir + element)\n",
    "    for item in doc_files:\n",
    "        with codecs.open(dir + element + '/' + item, encoding='utf-8', errors='ignore') as doc_file:\n",
    "            text = doc_file.read()\n",
    "        docs.append(text)\n",
    "        \n",
    "print len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below we are going to perform the needed preprocessing. To be more specific we are going to remove stopwords, transform all words to lowercase and remove punctuation. Then we lemmatize the words and keep only those whose length exceed the size of 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "countVectorizer = CountVectorizer()\n",
    "tfidfVectorizer = TfidfVectorizer()\n",
    "punctFree = re.compile('.*[A-Za-z].*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    token = nltk.word_tokenize(text)\n",
    "    lowerCase = [word.lower() for word in token]\n",
    "    raw_words = [tok for tok in lowerCase if punctFree.match(tok)]\n",
    "    filtering = list(filter(lambda l: l not in stop_words, raw_words))\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in filtering]\n",
    "    lemmas = [lemma for lemma in lemmas if len(lemma) > 2]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for text in docs:\n",
    "    tokens.append(cleaning(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to join the tokens of a doc and vectorize the words via term frequencies and via inverse document frequencies TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join the tokens of a paper to be able to vectorize the words\n",
    "joined_tokens = [\" \".join(item) for item in tokens]\n",
    "count_vectors = countVectorizer.fit_transform(joined_tokens)\n",
    "tfidf_vectors = tfidfVectorizer.fit_transform(joined_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use LDA we have to change the form of data. We are going to use the Dictionary method of gensim which will create the required form. In other words, we are goin to have a dictionary which contains (term, count of term in docs). Also we are going to remove the rare words because we assume that they are meaningless in comparison with the most frequent words. Finally, we check the form of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict = corpora.Dictionary(tokens)\n",
    "dict.filter_extremes(no_below=20, no_above=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dict.doc2bow(doc) for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1st', 2)\n",
      "(u'able', 2)\n",
      "(u'adjust', 1)\n",
      "(u'adjusted', 3)\n",
      "(u'algorithm', 3)\n",
      "(u'along', 2)\n",
      "(u'american', 1)\n",
      "(u'analysis', 3)\n",
      "(u'another', 1)\n",
      "(u'appeared', 1)\n",
      "(u'area', 6)\n",
      "(u'assigned', 1)\n",
      "(u'available', 3)\n",
      "(u'back', 22)\n",
      "(u'behavior', 1)\n",
      "(u'brain', 2)\n",
      "(u'change', 1)\n",
      "(u'combined', 4)\n",
      "(u'complex', 13)\n",
      "(u'computer', 1)\n",
      "(u'conclusion', 1)\n",
      "(u'conference', 4)\n",
      "(u'connected', 1)\n",
      "(u'connection', 4)\n",
      "(u'context', 1)\n",
      "(u'could', 1)\n",
      "(u'cybernetics', 1)\n",
      "(u'demonstrated', 2)\n",
      "(u'department', 1)\n",
      "(u'depends', 1)\n",
      "(u'descent', 1)\n",
      "(u'described', 3)\n",
      "(u'desired', 5)\n",
      "(u'detail', 1)\n",
      "(u'determine', 3)\n",
      "(u'diagram', 1)\n",
      "(u'difference', 2)\n",
      "(u'difficult', 3)\n",
      "(u'discrete', 1)\n",
      "(u'discus', 1)\n",
      "(u'distribution', 1)\n",
      "(u'enough', 1)\n",
      "(u'equation', 1)\n",
      "(u'error', 15)\n",
      "(u'estimate', 2)\n",
      "(u'fed', 1)\n",
      "(u'feedback', 1)\n",
      "(u'finite', 1)\n",
      "(u'fixed', 20)\n",
      "(u'fixing', 1)\n",
      "(u'form', 24)\n",
      "(u'formed', 16)\n",
      "(u'forming', 3)\n",
      "(u'four', 1)\n",
      "(u'framework', 2)\n",
      "(u'gain', 1)\n",
      "(u'general', 2)\n",
      "(u'generated', 2)\n",
      "(u'hidden', 15)\n",
      "(u'high', 5)\n",
      "(u'illustrated', 1)\n",
      "(u'illustrates', 1)\n",
      "(u'implement', 4)\n",
      "(u'implemented', 1)\n",
      "(u'include', 2)\n",
      "(u'indicated', 1)\n",
      "(u'institute', 1)\n",
      "(u'interaction', 2)\n",
      "(u'international', 4)\n",
      "(u'involves', 2)\n",
      "(u'june', 3)\n",
      "(u'laboratory', 1)\n",
      "(u'large', 3)\n",
      "(u'layer', 41)\n",
      "(u'learning', 5)\n",
      "(u'limit', 1)\n",
      "(u'linear', 2)\n",
      "(u'long', 3)\n",
      "(u'machine', 2)\n",
      "(u'many', 5)\n",
      "(u'may', 1)\n",
      "(u'mechanism', 1)\n",
      "(u'method', 1)\n",
      "(u'might', 2)\n",
      "(u'mit', 1)\n",
      "(u'much', 2)\n",
      "(u'multiple', 1)\n",
      "(u'must', 3)\n",
      "(u'necessary', 1)\n",
      "(u'needed', 1)\n",
      "(u'net', 12)\n",
      "(u'new', 2)\n",
      "(u'occurrence', 2)\n",
      "(u'often', 3)\n",
      "(u'organization', 1)\n",
      "(u'output', 26)\n",
      "(u'overall', 1)\n",
      "(u'paper', 3)\n",
      "(u'parallel', 1)\n",
      "(u'particular', 1)\n",
      "(u'perform', 1)\n",
      "(u'performed', 2)\n",
      "(u'physic', 1)\n",
      "(u'place', 1)\n",
      "(u'position', 1)\n",
      "(u'possible', 2)\n",
      "(u'presented', 7)\n",
      "(u'previous', 4)\n",
      "(u'probability', 4)\n",
      "(u'procedure', 2)\n",
      "(u'processing', 1)\n",
      "(u'propagation', 20)\n",
      "(u'provided', 7)\n",
      "(u'provides', 3)\n",
      "(u'quantity', 1)\n",
      "(u'rate', 7)\n",
      "(u'recent', 1)\n",
      "(u'recently', 1)\n",
      "(u'represent', 3)\n",
      "(u'representation', 3)\n",
      "(u'required', 10)\n",
      "(u'research', 1)\n",
      "(u'royal', 1)\n",
      "(u'second', 5)\n",
      "(u'section', 1)\n",
      "(u'see', 1)\n",
      "(u'seen', 2)\n",
      "(u'selected', 2)\n",
      "(u'signal', 1)\n",
      "(u'similar', 11)\n",
      "(u'simple', 2)\n",
      "(u'simulation', 7)\n",
      "(u'single', 1)\n",
      "(u'solution', 2)\n",
      "(u'speech', 5)\n",
      "(u'stage', 1)\n",
      "(u'state', 2)\n",
      "(u'stored', 1)\n",
      "(u'structure', 1)\n",
      "(u'suggest', 2)\n",
      "(u'symbol', 1)\n",
      "(u'take', 2)\n",
      "(u'technique', 1)\n",
      "(u'three', 5)\n",
      "(u'thus', 1)\n",
      "(u'tion', 1)\n",
      "(u'trained', 13)\n",
      "(u'training', 37)\n",
      "(u'type', 4)\n",
      "(u'typically', 1)\n",
      "(u'variable', 1)\n",
      "(u'varies', 1)\n",
      "(u'view', 1)\n",
      "(u'vol', 2)\n",
      "(u'weight', 33)\n",
      "(u'well', 4)\n",
      "(u'whether', 2)\n",
      "(u'within', 3)\n",
      "(u'work', 4)\n",
      "(u'zero', 1)\n",
      "(u'adapting', 1)\n",
      "(u'addition', 2)\n",
      "(u'advanced', 1)\n",
      "(u'amount', 1)\n",
      "(u'approach', 2)\n",
      "(u'architecture', 4)\n",
      "(u'attempt', 1)\n",
      "(u'best', 2)\n",
      "(u'class', 19)\n",
      "(u'classified', 1)\n",
      "(u'considered', 1)\n",
      "(u'consists', 1)\n",
      "(u'continuous', 1)\n",
      "(u'correct', 1)\n",
      "(u'corresponding', 4)\n",
      "(u'cross', 1)\n",
      "(u'data', 11)\n",
      "(u'determined', 1)\n",
      "(u'developed', 2)\n",
      "(u'expressed', 1)\n",
      "(u'fig', 33)\n",
      "(u'filled', 2)\n",
      "(u'generally', 1)\n",
      "(u'generate', 1)\n",
      "(u'half', 1)\n",
      "(u'head', 1)\n",
      "(u'hill', 1)\n",
      "(u'influence', 1)\n",
      "(u'known', 1)\n",
      "(u'left', 4)\n",
      "(u'line', 6)\n",
      "(u'map', 23)\n",
      "(u'n.y.', 1)\n",
      "(u'observation', 1)\n",
      "(u'obtained', 3)\n",
      "(u'pattern', 3)\n",
      "(u'project', 1)\n",
      "(u'provide', 6)\n",
      "(u'put', 1)\n",
      "(u'random', 5)\n",
      "(u'randomly', 1)\n",
      "(u'rapidly', 2)\n",
      "(u'reduce', 2)\n",
      "(u'region', 61)\n",
      "(u'respectively', 1)\n",
      "(u'restricted', 1)\n",
      "(u'resulting', 1)\n",
      "(u'right', 1)\n",
      "(u'sample', 4)\n",
      "(u'select', 3)\n",
      "(u'separate', 4)\n",
      "(u'significant', 2)\n",
      "(u'situation', 3)\n",
      "(u'small', 2)\n",
      "(u'space', 2)\n",
      "(u'specific', 1)\n",
      "(u'square', 1)\n",
      "(u'studied', 1)\n",
      "(u'test', 3)\n",
      "(u'together', 1)\n",
      "(u'trial', 10)\n",
      "(u'upper', 4)\n",
      "(u'7th', 1)\n",
      "(u'agency', 1)\n",
      "(u'air', 1)\n",
      "(u'albus', 2)\n",
      "(u'alter-', 1)\n",
      "(u'alternately', 1)\n",
      "(u'alternative', 3)\n",
      "(u'applied', 1)\n",
      "(u'april', 1)\n",
      "(u'arbitrary', 5)\n",
      "(u'arrow', 2)\n",
      "(u'associative', 1)\n",
      "(u'assp', 1)\n",
      "(u'asymptotic', 1)\n",
      "(u'attained', 1)\n",
      "(u'august', 1)\n",
      "(u'author', 1)\n",
      "(u'automatically', 1)\n",
      "(u'averaged', 1)\n",
      "(u'belong', 1)\n",
      "(u'better', 2)\n",
      "(u'bin', 1)\n",
      "(u'book', 3)\n",
      "(u'boolean', 2)\n",
      "(u'bottom', 1)\n",
      "(u'boundary', 5)\n",
      "(u'break', 1)\n",
      "(u'burr', 1)\n",
      "(u'calculate', 1)\n",
      "(u'called', 2)\n",
      "(u'capability', 3)\n",
      "(u'carefully', 1)\n",
      "(u'carlo', 3)\n",
      "(u'child', 1)\n",
      "(u'circular', 1)\n",
      "(u'clas-', 1)\n",
      "(u'classi-', 1)\n",
      "(u'classification', 4)\n",
      "(u'classifier', 86)\n",
      "(u'clustering', 1)\n",
      "(u'cmac', 1)\n",
      "(u'com-', 1)\n",
      "(u'comparative', 2)\n",
      "(u'comparing', 1)\n",
      "(u'comparison', 2)\n",
      "(u'completely', 1)\n",
      "(u'computing', 1)\n",
      "(u'confer-', 1)\n",
      "(u'connec-', 1)\n",
      "(u'construct', 3)\n",
      "(u'constructive', 1)\n",
      "(u'contain', 1)\n",
      "(u'contains', 1)\n",
      "(u'continuous-valued', 3)\n",
      "(u'conventional', 2)\n",
      "(u'converge', 1)\n",
      "(u'convergence', 15)\n",
      "(u'convex', 7)\n",
      "(u'cooper', 1)\n",
      "(u'correlation', 1)\n",
      "(u'counting', 1)\n",
      "(u'cover', 2)\n",
      "(u'covered', 1)\n",
      "(u'covering', 1)\n",
      "(u'created', 2)\n",
      "(u'curve', 4)\n",
      "(u'dashed', 3)\n",
      "(u'deci-', 1)\n",
      "(u'decision', 53)\n",
      "(u'defense', 1)\n",
      "(u'defined', 1)\n",
      "(u'definition', 1)\n",
      "(u'demon-', 1)\n",
      "(u'demonstrate', 5)\n",
      "(u'density', 5)\n",
      "(u'designed', 1)\n",
      "(u'despite', 1)\n",
      "(u'digit', 1)\n",
      "(u'disjoint', 11)\n",
      "(u'divide', 1)\n",
      "(u'domain', 3)\n",
      "(u'drawn', 1)\n",
      "(u'duda', 1)\n",
      "(u'eliminate', 1)\n",
      "(u'eliminated', 1)\n",
      "(u'ence', 1)\n",
      "(u'entire', 1)\n",
      "(u'entry', 1)\n",
      "(u'equal', 1)\n",
      "(u'establishment', 1)\n",
      "(u'eval-', 1)\n",
      "(u'except', 3)\n",
      "(u'excessive', 1)\n",
      "(u'exemplar', 2)\n",
      "(u'explore', 1)\n",
      "(u'explored', 1)\n",
      "(u'extend', 1)\n",
      "(u'extensively', 1)\n",
      "(u'extremely', 1)\n",
      "(u'failed', 2)\n",
      "(u'fall', 2)\n",
      "(u'faster', 4)\n",
      "(u'feature', 24)\n",
      "(u'fewer', 3)\n",
      "(u'filtered', 1)\n",
      "(u'final', 2)\n",
      "(u'first-', 1)\n",
      "(u'flexible', 1)\n",
      "(u'force', 1)\n",
      "(u'formant', 3)\n",
      "(u'freeman', 1)\n",
      "(u'frequency', 1)\n",
      "(u'fully', 1)\n",
      "(u'furthermore', 1)\n",
      "(u'gaussian', 3)\n",
      "(u'generative', 2)\n",
      "(u'geometric', 1)\n",
      "(u'gold', 1)\n",
      "(u'goldstein', 1)\n",
      "(u'good', 5)\n",
      "(u'government', 1)\n",
      "(u'greatest', 1)\n",
      "(u'grid', 8)\n",
      "(u'guarantee', 1)\n",
      "(u'hand', 2)\n",
      "(u'hart', 1)\n",
      "(u'histogram', 4)\n",
      "(u'hood', 1)\n",
      "(u'huang', 2)\n",
      "(u'hypercube', 14)\n",
      "(u'hyperplane', 2)\n",
      "(u'hyperplanes', 18)\n",
      "(u'ieee', 7)\n",
      "(u'illustrate', 1)\n",
      "(u'illustrative', 1)\n",
      "(u'image', 1)\n",
      "(u'improve', 1)\n",
      "(u'improved', 1)\n",
      "(u'improvement', 2)\n",
      "(u'in-', 1)\n",
      "(u'indicate', 1)\n",
      "(u'ing', 1)\n",
      "(u'initialized', 1)\n",
      "(u'integrator', 1)\n",
      "(u'interact', 1)\n",
      "(u'interna-', 1)\n",
      "(u'inversion', 1)\n",
      "(u'issue', 1)\n",
      "(u'ist', 1)\n",
      "(u'july', 1)\n",
      "(u'k-nearest', 10)\n",
      "(u'know', 1)\n",
      "(u'kohonen', 3)\n",
      "(u'label', 2)\n",
      "(u'labeled', 1)\n",
      "(u'larger', 1)\n",
      "(u'lay', 1)\n",
      "(u'led', 2)\n",
      "(u'likelihood', 5)\n",
      "(u'lincoln', 1)\n",
      "(u'linearly', 1)\n",
      "(u'lippmann', 4)\n",
      "(u'little', 3)\n",
      "(u'low', 1)\n",
      "(u'low-pass', 1)\n",
      "(u'magazine', 1)\n",
      "(u'majority', 1)\n",
      "(u'man', 1)\n",
      "(u'mance', 1)\n",
      "(u'markov', 1)\n",
      "(u'maximum', 10)\n",
      "(u'mcgraw-hill', 2)\n",
      "(u'meaning', 1)\n",
      "(u'mentioned', 1)\n",
      "(u'minor', 1)\n",
      "(u'modified', 1)\n",
      "(u'momentum', 1)\n",
      "(u'monte', 3)\n",
      "(u'multi', 1)\n",
      "(u'multi-layer', 9)\n",
      "(u'n.j.', 1)\n",
      "(u'native', 1)\n",
      "(u'near', 4)\n",
      "(u'neighbor', 10)\n",
      "(u'never', 1)\n",
      "(u'nine', 1)\n",
      "(u'nique', 1)\n",
      "(u'node', 48)\n",
      "(u'non-zero', 1)\n",
      "(u'nonlinearities', 3)\n",
      "(u'noted', 1)\n",
      "(u'november', 1)\n",
      "(u'obtain', 1)\n",
      "(u'offset', 5)\n",
      "(u'organizing', 1)\n",
      "(u'outcome', 1)\n",
      "(u'page', 1)\n",
      "(u'per', 1)\n",
      "(u'per-', 1)\n",
      "(u'percent', 1)\n",
      "(u'percentage', 4)\n",
      "(u'perceptron', 9)\n",
      "(u'perceptrons', 12)\n",
      "(u'perceptton', 9)\n",
      "(u'percepttons', 8)\n",
      "(u'perfor-', 1)\n",
      "(u'performance', 14)\n",
      "(u'peterson', 2)\n",
      "(u'placement', 1)\n",
      "(u'plane', 1)\n",
      "(u'point', 6)\n",
      "(u'policy', 1)\n",
      "(u'poor', 1)\n",
      "(u'proceeding', 2)\n",
      "(u'proof', 4)\n",
      "(u'proportional', 1)\n",
      "(u'proving', 1)\n",
      "(u'r/=', 1)\n",
      "(u'radar', 1)\n",
      "(u'rapid', 1)\n",
      "(u'recognition', 5)\n",
      "(u'recognizer', 1)\n",
      "(u'reflect', 1)\n",
      "(u'remain', 1)\n",
      "(u'remaining', 1)\n",
      "(u'removed', 1)\n",
      "(u'reported', 1)\n",
      "(u'represents', 1)\n",
      "(u'rest', 1)\n",
      "(u'richard', 1)\n",
      "(u'robotics', 1)\n",
      "(u'robust', 2)\n",
      "(u'run', 2)\n",
      "(u'scene', 1)\n",
      "(u'selecting', 1)\n",
      "(u'self', 1)\n",
      "(u'self-', 1)\n",
      "(u'self-organizing', 1)\n",
      "(u'sensitivity', 1)\n",
      "(u'separability', 2)\n",
      "(u'separable', 1)\n",
      "(u'separated', 2)\n",
      "(u'separating', 1)\n",
      "(u'shaded', 4)\n",
      "(u'shape', 1)\n",
      "(u'shorter', 1)\n",
      "(u'sigmoidal', 1)\n",
      "(u'sign', 1)\n",
      "(u'simpler', 1)\n",
      "(u'simply', 1)\n",
      "(u'simula-', 1)\n",
      "(u'sion', 1)\n",
      "(u'slightly', 2)\n",
      "(u'slope', 1)\n",
      "(u'slow', 1)\n",
      "(u'slower', 1)\n",
      "(u'smooth', 1)\n",
      "(u'smoothly', 1)\n",
      "(u'solid', 1)\n",
      "(u'sometimes', 1)\n",
      "(u'son', 1)\n",
      "(u'split', 1)\n",
      "(u'spoken', 1)\n",
      "(u'sponsored', 1)\n",
      "(u'steady', 2)\n",
      "(u'strated', 1)\n",
      "(u'strongest', 1)\n",
      "(u'substantially', 1)\n",
      "(u'suggests', 1)\n",
      "(u'summarizes', 1)\n",
      "(u'summing', 1)\n",
      "(u'supervised', 9)\n",
      "(u'supervision', 1)\n",
      "(u'table', 2)\n",
      "(u'tech-', 1)\n",
      "(u'ten', 4)\n",
      "(u'term', 4)\n",
      "(u'tested', 4)\n",
      "(u'testing', 2)\n",
      "(u'theory', 1)\n",
      "(u'thick', 1)\n",
      "(u'third', 1)\n",
      "(u'three-layer', 7)\n",
      "(u'tially', 1)\n",
      "(u'tional', 1)\n",
      "(u'tions', 1)\n",
      "(u'token', 10)\n",
      "(u'top', 4)\n",
      "(u'total', 1)\n",
      "(u'traditional', 4)\n",
      "(u'train', 4)\n",
      "(u'train-', 1)\n",
      "(u'trainable', 1)\n",
      "(u'transaction', 1)\n",
      "(u'triangular', 1)\n",
      "(u'twice', 1)\n",
      "(u'two-layer', 23)\n",
      "(u'u.s.', 1)\n",
      "(u'unchanged', 1)\n",
      "(u'understanding', 1)\n",
      "(u'uniform', 1)\n",
      "(u'unlabeled', 3)\n",
      "(u'unsupervised', 7)\n",
      "(u'untrained', 1)\n",
      "(u'usa', 1)\n",
      "(u'useful', 1)\n",
      "(u'valued', 1)\n",
      "(u'versa', 1)\n",
      "(u'vice', 1)\n",
      "(u'voice', 1)\n",
      "(u'vowel', 10)\n",
      "(u'vs.', 2)\n",
      "(u'washington', 1)\n",
      "(u'whereas', 1)\n",
      "(u'william', 1)\n",
      "(u'worse', 1)\n",
      "(u'york', 2)\n"
     ]
    }
   ],
   "source": [
    "for i in corpus[2]:\n",
    "    print (dict[i[0]], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA do not give attention to the order of words so we are going to use the bag of words. Each document is a distribution over topics. Each topic is a distribution over words which belong to the vocabulary. \n",
    "So we are going to choose a distribution over topics, draw a topic and choose words from the topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use different number of topics and change the a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num_topics = 10\n",
    "## alpha = auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.010*\"learning\" + 0.007*\"data\" + 0.007*\"algorithm\" + 0.006*\"unit\" + 0.005*\"state\" + 0.005*\"error\" + 0.004*\"image\" + 0.004*\"training\" + 0.004*\"parameter\" + 0.004*\"point\"'),\n",
       " (1,\n",
       "  u'0.010*\"learning\" + 0.008*\"data\" + 0.005*\"pattern\" + 0.005*\"neuron\" + 0.005*\"training\" + 0.005*\"unit\" + 0.004*\"algorithm\" + 0.004*\"state\" + 0.004*\"output\" + 0.004*\"vector\"'),\n",
       " (2,\n",
       "  u'0.008*\"learning\" + 0.006*\"algorithm\" + 0.005*\"parameter\" + 0.005*\"training\" + 0.005*\"image\" + 0.005*\"weight\" + 0.005*\"method\" + 0.005*\"neuron\" + 0.004*\"unit\" + 0.004*\"error\"'),\n",
       " (3,\n",
       "  u'0.009*\"learning\" + 0.008*\"weight\" + 0.008*\"output\" + 0.008*\"algorithm\" + 0.008*\"unit\" + 0.006*\"training\" + 0.006*\"data\" + 0.005*\"error\" + 0.005*\"vector\" + 0.004*\"neuron\"'),\n",
       " (4,\n",
       "  u'0.007*\"learning\" + 0.007*\"unit\" + 0.007*\"data\" + 0.006*\"output\" + 0.005*\"weight\" + 0.005*\"training\" + 0.005*\"algorithm\" + 0.004*\"error\" + 0.004*\"pattern\" + 0.004*\"vector\"'),\n",
       " (5,\n",
       "  u'0.009*\"data\" + 0.007*\"learning\" + 0.007*\"algorithm\" + 0.006*\"training\" + 0.006*\"unit\" + 0.006*\"error\" + 0.005*\"output\" + 0.004*\"weight\" + 0.004*\"state\" + 0.004*\"vector\"'),\n",
       " (6,\n",
       "  u'0.007*\"learning\" + 0.007*\"algorithm\" + 0.006*\"method\" + 0.005*\"data\" + 0.004*\"training\" + 0.004*\"error\" + 0.004*\"state\" + 0.004*\"parameter\" + 0.004*\"weight\" + 0.004*\"neuron\"'),\n",
       " (7,\n",
       "  u'0.007*\"learning\" + 0.006*\"unit\" + 0.006*\"training\" + 0.005*\"data\" + 0.005*\"algorithm\" + 0.005*\"image\" + 0.005*\"state\" + 0.005*\"vector\" + 0.004*\"neuron\" + 0.004*\"performance\"'),\n",
       " (8,\n",
       "  u'0.009*\"learning\" + 0.006*\"state\" + 0.006*\"algorithm\" + 0.006*\"training\" + 0.005*\"unit\" + 0.005*\"output\" + 0.005*\"weight\" + 0.005*\"data\" + 0.004*\"method\" + 0.004*\"neuron\"'),\n",
       " (9,\n",
       "  u'0.008*\"learning\" + 0.006*\"data\" + 0.006*\"training\" + 0.005*\"algorithm\" + 0.005*\"error\" + 0.005*\"neuron\" + 0.005*\"output\" + 0.005*\"cell\" + 0.005*\"state\" + 0.004*\"unit\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_10_auto = models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                          id2word=dict, \n",
    "                                          num_topics=10,\n",
    "                                          alpha='auto')\n",
    "lda_10_auto.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the topic 0 we can see that the 10 most important by contribution to the topic are:\n",
    "weight  (0.007)\n",
    "data (0.007)  \n",
    "output  (0.007)\n",
    "learning (0.007)\n",
    "unit  (0.007)\n",
    "neuron  (0.005) \n",
    "training   (0.005)\n",
    "cell (0.005)  \n",
    "feature  (0.005)\n",
    "vector (0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use perplexity and coherence as measures to evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.7831372701548123"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_10_auto.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29087387861489111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_10_auto = CoherenceModel(model=lda_10_auto, texts=tokens, dictionary=dict, coherence='c_v')\n",
    "coherence_10_auto.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num_topics = 20\n",
    "## alpha = auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.009*\"unit\" + 0.006*\"training\" + 0.005*\"neuron\" + 0.005*\"output\" + 0.005*\"data\" + 0.004*\"representation\" + 0.004*\"pattern\" + 0.004*\"feature\" + 0.004*\"error\" + 0.004*\"algorithm\"'),\n",
       " (1,\n",
       "  u'0.009*\"learning\" + 0.008*\"data\" + 0.006*\"output\" + 0.005*\"training\" + 0.004*\"weight\" + 0.004*\"feature\" + 0.004*\"algorithm\" + 0.004*\"state\" + 0.004*\"probability\" + 0.004*\"unit\"'),\n",
       " (2,\n",
       "  u'0.010*\"learning\" + 0.007*\"unit\" + 0.007*\"training\" + 0.006*\"data\" + 0.006*\"output\" + 0.005*\"image\" + 0.005*\"hidden\" + 0.004*\"method\" + 0.004*\"error\" + 0.004*\"weight\"'),\n",
       " (3,\n",
       "  u'0.009*\"learning\" + 0.008*\"data\" + 0.006*\"error\" + 0.006*\"state\" + 0.005*\"training\" + 0.005*\"unit\" + 0.005*\"method\" + 0.005*\"algorithm\" + 0.004*\"weight\" + 0.004*\"output\"'),\n",
       " (4,\n",
       "  u'0.010*\"neuron\" + 0.008*\"unit\" + 0.008*\"learning\" + 0.007*\"output\" + 0.006*\"layer\" + 0.005*\"weight\" + 0.005*\"data\" + 0.005*\"error\" + 0.004*\"cell\" + 0.004*\"training\"'),\n",
       " (5,\n",
       "  u'0.008*\"training\" + 0.008*\"learning\" + 0.007*\"weight\" + 0.006*\"error\" + 0.005*\"algorithm\" + 0.005*\"unit\" + 0.005*\"output\" + 0.005*\"vector\" + 0.005*\"data\" + 0.004*\"method\"'),\n",
       " (6,\n",
       "  u'0.008*\"algorithm\" + 0.008*\"data\" + 0.006*\"learning\" + 0.005*\"weight\" + 0.005*\"unit\" + 0.004*\"point\" + 0.004*\"feature\" + 0.004*\"state\" + 0.004*\"neuron\" + 0.003*\"probability\"'),\n",
       " (7,\n",
       "  u'0.013*\"learning\" + 0.013*\"algorithm\" + 0.007*\"training\" + 0.005*\"data\" + 0.005*\"error\" + 0.004*\"method\" + 0.004*\"distribution\" + 0.004*\"vector\" + 0.004*\"space\" + 0.004*\"unit\"'),\n",
       " (8,\n",
       "  u'0.006*\"neuron\" + 0.006*\"cell\" + 0.005*\"weight\" + 0.005*\"output\" + 0.005*\"learning\" + 0.005*\"algorithm\" + 0.005*\"data\" + 0.005*\"image\" + 0.005*\"vector\" + 0.005*\"pattern\"'),\n",
       " (9,\n",
       "  u'0.007*\"learning\" + 0.007*\"algorithm\" + 0.006*\"image\" + 0.006*\"data\" + 0.005*\"parameter\" + 0.005*\"error\" + 0.005*\"training\" + 0.005*\"state\" + 0.004*\"method\" + 0.004*\"unit\"'),\n",
       " (10,\n",
       "  u'0.010*\"learning\" + 0.009*\"state\" + 0.008*\"weight\" + 0.006*\"unit\" + 0.006*\"output\" + 0.006*\"data\" + 0.006*\"vector\" + 0.005*\"pattern\" + 0.005*\"algorithm\" + 0.004*\"neuron\"'),\n",
       " (11,\n",
       "  u'0.008*\"learning\" + 0.007*\"algorithm\" + 0.006*\"data\" + 0.006*\"error\" + 0.005*\"point\" + 0.005*\"unit\" + 0.005*\"training\" + 0.004*\"state\" + 0.004*\"output\" + 0.004*\"weight\"'),\n",
       " (12,\n",
       "  u'0.012*\"learning\" + 0.008*\"data\" + 0.007*\"pattern\" + 0.007*\"training\" + 0.006*\"algorithm\" + 0.005*\"output\" + 0.005*\"neuron\" + 0.005*\"unit\" + 0.004*\"error\" + 0.004*\"weight\"'),\n",
       " (13,\n",
       "  u'0.008*\"data\" + 0.007*\"learning\" + 0.006*\"algorithm\" + 0.006*\"image\" + 0.005*\"training\" + 0.005*\"point\" + 0.005*\"state\" + 0.004*\"neuron\" + 0.004*\"unit\" + 0.004*\"weight\"'),\n",
       " (14,\n",
       "  u'0.011*\"data\" + 0.008*\"training\" + 0.005*\"algorithm\" + 0.005*\"learning\" + 0.005*\"state\" + 0.005*\"parameter\" + 0.004*\"cell\" + 0.004*\"unit\" + 0.004*\"error\" + 0.004*\"performance\"'),\n",
       " (15,\n",
       "  u'0.009*\"learning\" + 0.006*\"output\" + 0.006*\"training\" + 0.005*\"data\" + 0.005*\"error\" + 0.005*\"weight\" + 0.005*\"algorithm\" + 0.005*\"unit\" + 0.005*\"state\" + 0.004*\"vector\"'),\n",
       " (16,\n",
       "  u'0.010*\"learning\" + 0.008*\"unit\" + 0.007*\"training\" + 0.006*\"algorithm\" + 0.006*\"weight\" + 0.005*\"parameter\" + 0.005*\"output\" + 0.005*\"method\" + 0.004*\"vector\" + 0.004*\"hidden\"'),\n",
       " (17,\n",
       "  u'0.007*\"unit\" + 0.007*\"learning\" + 0.007*\"algorithm\" + 0.007*\"output\" + 0.006*\"error\" + 0.006*\"weight\" + 0.006*\"training\" + 0.005*\"vector\" + 0.005*\"data\" + 0.004*\"pattern\"'),\n",
       " (18,\n",
       "  u'0.010*\"algorithm\" + 0.006*\"learning\" + 0.005*\"unit\" + 0.005*\"state\" + 0.005*\"error\" + 0.005*\"training\" + 0.004*\"pattern\" + 0.004*\"weight\" + 0.004*\"output\" + 0.004*\"performance\"'),\n",
       " (19,\n",
       "  u'0.011*\"learning\" + 0.007*\"unit\" + 0.006*\"state\" + 0.006*\"weight\" + 0.006*\"algorithm\" + 0.006*\"output\" + 0.006*\"training\" + 0.005*\"pattern\" + 0.004*\"error\" + 0.004*\"cell\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_20_auto = models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                          id2word=dict, \n",
    "                                          num_topics=20,\n",
    "                                          alpha='auto')\n",
    "lda_20_auto.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.8539139338855959"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_20_auto.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2987799304345764"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_20_auto = CoherenceModel(model=lda_20_auto, texts=tokens, dictionary=dict, coherence='c_v')\n",
    "coherence_20_auto.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num_topics = 20\n",
    "## alpha = assymetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to keep the second case because we have observed that the number of perplexity is lower. In the following chapters we are going to change the parameter a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.009*\"learning\" + 0.008*\"data\" + 0.007*\"error\" + 0.006*\"state\" + 0.006*\"algorithm\" + 0.006*\"unit\" + 0.005*\"method\" + 0.005*\"weight\" + 0.004*\"output\" + 0.004*\"training\"'),\n",
       " (1,\n",
       "  u'0.006*\"learning\" + 0.006*\"unit\" + 0.005*\"data\" + 0.005*\"weight\" + 0.004*\"method\" + 0.004*\"error\" + 0.004*\"output\" + 0.004*\"feature\" + 0.004*\"training\" + 0.004*\"vector\"'),\n",
       " (2,\n",
       "  u'0.010*\"learning\" + 0.008*\"data\" + 0.008*\"unit\" + 0.005*\"algorithm\" + 0.005*\"output\" + 0.005*\"error\" + 0.004*\"neuron\" + 0.004*\"image\" + 0.004*\"method\" + 0.004*\"layer\"'),\n",
       " (3,\n",
       "  u'0.013*\"learning\" + 0.008*\"unit\" + 0.008*\"output\" + 0.008*\"training\" + 0.007*\"weight\" + 0.006*\"state\" + 0.006*\"algorithm\" + 0.005*\"data\" + 0.005*\"error\" + 0.005*\"pattern\"'),\n",
       " (4,\n",
       "  u'0.009*\"learning\" + 0.008*\"data\" + 0.008*\"training\" + 0.007*\"output\" + 0.007*\"unit\" + 0.006*\"algorithm\" + 0.005*\"class\" + 0.005*\"hidden\" + 0.004*\"state\" + 0.004*\"vector\"'),\n",
       " (5,\n",
       "  u'0.011*\"learning\" + 0.008*\"algorithm\" + 0.008*\"unit\" + 0.007*\"error\" + 0.006*\"weight\" + 0.006*\"point\" + 0.005*\"data\" + 0.005*\"training\" + 0.005*\"output\" + 0.004*\"vector\"'),\n",
       " (6,\n",
       "  u'0.007*\"neuron\" + 0.007*\"learning\" + 0.005*\"cell\" + 0.005*\"algorithm\" + 0.005*\"data\" + 0.004*\"pattern\" + 0.004*\"output\" + 0.004*\"training\" + 0.004*\"state\" + 0.003*\"vector\"'),\n",
       " (7,\n",
       "  u'0.011*\"learning\" + 0.011*\"algorithm\" + 0.005*\"data\" + 0.005*\"output\" + 0.004*\"state\" + 0.004*\"layer\" + 0.004*\"image\" + 0.004*\"training\" + 0.004*\"pattern\" + 0.004*\"point\"'),\n",
       " (8,\n",
       "  u'0.009*\"unit\" + 0.008*\"data\" + 0.006*\"algorithm\" + 0.006*\"output\" + 0.005*\"learning\" + 0.005*\"state\" + 0.005*\"vector\" + 0.005*\"error\" + 0.004*\"hidden\" + 0.004*\"neuron\"'),\n",
       " (9,\n",
       "  u'0.009*\"learning\" + 0.009*\"state\" + 0.006*\"algorithm\" + 0.005*\"data\" + 0.005*\"weight\" + 0.005*\"training\" + 0.004*\"neuron\" + 0.003*\"parameter\" + 0.003*\"output\" + 0.003*\"method\"'),\n",
       " (10,\n",
       "  u'0.009*\"learning\" + 0.008*\"unit\" + 0.008*\"training\" + 0.006*\"data\" + 0.006*\"error\" + 0.006*\"algorithm\" + 0.005*\"pattern\" + 0.005*\"output\" + 0.004*\"weight\" + 0.004*\"vector\"'),\n",
       " (11,\n",
       "  u'0.010*\"learning\" + 0.009*\"training\" + 0.008*\"error\" + 0.008*\"data\" + 0.007*\"weight\" + 0.006*\"unit\" + 0.006*\"algorithm\" + 0.005*\"pattern\" + 0.005*\"performance\" + 0.005*\"vector\"'),\n",
       " (12,\n",
       "  u'0.008*\"algorithm\" + 0.006*\"training\" + 0.006*\"learning\" + 0.006*\"data\" + 0.005*\"pattern\" + 0.004*\"point\" + 0.004*\"probability\" + 0.004*\"class\" + 0.004*\"feature\" + 0.004*\"vector\"'),\n",
       " (13,\n",
       "  u'0.009*\"neuron\" + 0.007*\"learning\" + 0.006*\"training\" + 0.006*\"algorithm\" + 0.005*\"output\" + 0.005*\"unit\" + 0.005*\"pattern\" + 0.005*\"weight\" + 0.005*\"data\" + 0.004*\"cell\"'),\n",
       " (14,\n",
       "  u'0.008*\"learning\" + 0.008*\"data\" + 0.005*\"probability\" + 0.005*\"unit\" + 0.004*\"weight\" + 0.004*\"algorithm\" + 0.004*\"method\" + 0.004*\"state\" + 0.004*\"error\" + 0.004*\"node\"'),\n",
       " (15,\n",
       "  u'0.009*\"algorithm\" + 0.008*\"learning\" + 0.007*\"state\" + 0.006*\"data\" + 0.005*\"parameter\" + 0.005*\"vector\" + 0.005*\"method\" + 0.004*\"probability\" + 0.004*\"image\" + 0.004*\"distribution\"'),\n",
       " (16,\n",
       "  u'0.009*\"learning\" + 0.007*\"weight\" + 0.006*\"feature\" + 0.006*\"algorithm\" + 0.006*\"unit\" + 0.005*\"output\" + 0.005*\"training\" + 0.005*\"neuron\" + 0.005*\"cell\" + 0.004*\"data\"'),\n",
       " (17,\n",
       "  u'0.006*\"learning\" + 0.006*\"neuron\" + 0.005*\"weight\" + 0.005*\"output\" + 0.005*\"data\" + 0.004*\"state\" + 0.004*\"probability\" + 0.004*\"parameter\" + 0.004*\"cell\" + 0.004*\"method\"'),\n",
       " (18,\n",
       "  u'0.009*\"learning\" + 0.009*\"training\" + 0.008*\"data\" + 0.006*\"algorithm\" + 0.006*\"weight\" + 0.005*\"pattern\" + 0.005*\"error\" + 0.005*\"vector\" + 0.005*\"unit\" + 0.004*\"output\"'),\n",
       " (19,\n",
       "  u'0.008*\"output\" + 0.007*\"unit\" + 0.006*\"weight\" + 0.006*\"data\" + 0.005*\"error\" + 0.005*\"image\" + 0.005*\"learning\" + 0.005*\"training\" + 0.005*\"method\" + 0.004*\"neuron\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_20_assymetric = models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                          id2word=dict, \n",
    "                                          num_topics=20, \n",
    "                                          alpha='asymmetric')\n",
    "lda_20_assymetric.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.8586099595881613"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_20_assymetric.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29291805017452177"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_20_assymetric = CoherenceModel(model=lda_20_assymetric, texts=tokens, dictionary=dict, coherence='c_v')\n",
    "coherence_20_assymetric.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates the visualization of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualization = pyLDAvis.gensim.prepare(lda_20_auto, corpus, dict)\n",
    "pyLDAvis.save_html(visualization, \"visualization.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the strongest topic representation for each one of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 0.44820261), 0),\n",
       " ((8, 0.5848825), 1),\n",
       " ((5, 0.79795474), 2),\n",
       " ((9, 0.48610616), 3),\n",
       " ((4, 0.22923909), 4),\n",
       " ((4, 0.30523324), 5),\n",
       " ((4, 0.69495332), 6),\n",
       " ((4, 0.45695239), 7),\n",
       " ((4, 0.58555472), 8),\n",
       " ((19, 0.8064993), 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_results = lda_20_auto[corpus]\n",
    "\n",
    "best = []\n",
    "i = 0\n",
    "\n",
    "for res in lda_results:\n",
    "    bs = max(res, key=lambda item:item[1])\n",
    "    best.append((bs, i))\n",
    "    i+=1\n",
    "best[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will find the paper that best represent the topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 0.9984014), 1053)\n",
      "((1, 0.96706814), 1588)\n",
      "((2, 0.93216282), 526)\n",
      "((3, 0.96038526), 1572)\n",
      "((4, 0.98709542), 1672)\n",
      "((5, 0.99657702), 1050)\n",
      "((6, 0.99560124), 665)\n",
      "((7, 0.99865419), 1247)\n",
      "((8, 0.99478024), 696)\n",
      "((9, 0.98445565), 1512)\n"
     ]
    }
   ],
   "source": [
    "topics = [i for i in range(10)]\n",
    "\n",
    "for item in topics:\n",
    "    fmax = []\n",
    "    for res in best:\n",
    "        if res[0][0] == item:\n",
    "            fmax.append(res)\n",
    "    #print fmax\n",
    "    if not fmax:\n",
    "        print 'None'\n",
    "    else:\n",
    "        res =max(fmax, key=lambda x:x[0][1])\n",
    "        print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
